{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analytics Dashboard\n",
    "\n",
    "This notebook provides comprehensive analytics for document processing evaluation results stored in the ReportingDatabase.\n",
    "\n",
    "## Features:\n",
    "- Connect to Athena and load partitions for evaluation tables\n",
    "- Query and analyze document, section, and attribute-level evaluation metrics\n",
    "- Generate reports highlighting accuracy problems and trends\n",
    "- Track evaluation accuracy across different documents and experiments over time\n",
    "\n",
    "## Prerequisites:\n",
    "1. Ensure you have the ReportingDatabase name from your CloudFormation stack outputs\n",
    "2. Configure AWS credentials with access to Athena and S3\n",
    "3. Install required dependencies: `pip install boto3 pandas matplotlib seaborn awswrangler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "try:\n",
    "    import awswrangler as wr\n",
    "except ImportError:\n",
    "    print(\"Installing awswrangler...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"awswrangler\"])\n",
    "    import awswrangler as wr\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these values with your stack outputs\n",
    "REPORTING_DATABASE = \"your-stack-name-evaluation-db\"  # From CloudFormation ReportingDatabase output\n",
    "AWS_REGION = \"us-east-1\"  # Your AWS region\n",
    "S3_RESULTS_BUCKET = \"aws-athena-query-results-your-account-region\"  # Your Athena results bucket\n",
    "\n",
    "# Table names in the ReportingDatabase\n",
    "DOCUMENT_TABLE = \"document_evaluations\"\n",
    "SECTION_TABLE = \"section_evaluations\"\n",
    "ATTRIBUTE_TABLE = \"attribute_evaluations\"\n",
    "\n",
    "# Analysis parameters\n",
    "START_DATE = \"2024-01-01\"  # Format: YYYY-MM-DD or None for no filter\n",
    "END_DATE = None  # Format: YYYY-MM-DD or None for no filter\n",
    "DOCUMENT_NAME_FILTER = None  # e.g., \"lending\" to filter documents containing \"lending\"\n",
    "LOW_ACCURACY_THRESHOLD = 0.8  # Documents/sections/attributes below this are flagged\n",
    "HIGH_ACCURACY_THRESHOLD = 0.95  # Documents/sections/attributes above this are excellent\n",
    "TOP_N_ITEMS = 10  # Number of top/bottom items to show in reports\n",
    "\n",
    "# Initialize AWS clients\n",
    "session = boto3.Session(region_name=AWS_REGION)\n",
    "athena_client = session.client('athena')\n",
    "\n",
    "print(f\"Configuration loaded for database: {REPORTING_DATABASE}\")\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "print(f\"Analysis parameters:\")\n",
    "print(f\"  - Low accuracy threshold: {LOW_ACCURACY_THRESHOLD}\")\n",
    "print(f\"  - High accuracy threshold: {HIGH_ACCURACY_THRESHOLD}\")\n",
    "print(f\"  - Date range: {START_DATE} to {END_DATE or 'present'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partitions_for_table(database_name, table_name):\n",
    "    \"\"\"Load partitions for a given table using MSCK REPAIR TABLE\"\"\"\n",
    "    query = f\"MSCK REPAIR TABLE {database_name}.{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            ResultConfiguration={'OutputLocation': f's3://{S3_RESULTS_BUCKET}/'},\n",
    "            WorkGroup='primary'\n",
    "        )\n",
    "        \n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        \n",
    "        while True:\n",
    "            result = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "            status = result['QueryExecution']['Status']['State']\n",
    "            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "        \n",
    "        if status == 'SUCCEEDED':\n",
    "            print(f\"✅ Successfully loaded partitions for {table_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            reason = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')\n",
    "            print(f\"❌ Failed to load partitions for {table_name}: {reason}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading partitions for {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def execute_athena_query(query):\n",
    "    \"\"\"Execute an Athena query and return results as a pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        return wr.athena.read_sql_query(sql=query, s3_output=f\"s3://{S3_RESULTS_BUCKET}/\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_where_clause():\n",
    "    \"\"\"Build WHERE clause based on filters\"\"\"\n",
    "    conditions = []\n",
    "    \n",
    "    if START_DATE:\n",
    "        conditions.append(f\"evaluation_date >= TIMESTAMP '{START_DATE} 00:00:00'\")\n",
    "    if END_DATE:\n",
    "        conditions.append(f\"evaluation_date <= TIMESTAMP '{END_DATE} 23:59:59'\")\n",
    "    if DOCUMENT_NAME_FILTER:\n",
    "        conditions.append(f\"LOWER(document_id) LIKE '%{DOCUMENT_NAME_FILTER.lower()}%'\")\n",
    "    \n",
    "    return \"WHERE \" + \" AND \".join(conditions) if conditions else \"\"\n",
    "\n",
    "print(\"Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load partitions for all tables\n",
    "print(\"🔄 Loading partitions for evaluation tables...\")\n",
    "tables = [DOCUMENT_TABLE, SECTION_TABLE, ATTRIBUTE_TABLE]\n",
    "\n",
    "for table in tables:\n",
    "    load_partitions_for_table(REPORTING_DATABASE, table)\n",
    "\n",
    "print(\"\\n✅ Partition loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Table Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connectivity and show table schemas\n",
    "print(\"📊 Testing table connectivity and showing schemas:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"\\n🔍 {table.upper()} TABLE:\")\n",
    "    \n",
    "    schema_query = f\"DESCRIBE {REPORTING_DATABASE}.{table}\"\n",
    "    schema_df = execute_athena_query(schema_query)\n",
    "    \n",
    "    if schema_df is not None:\n",
    "        print(\"Schema:\")\n",
    "        display(schema_df)\n",
    "        \n",
    "        count_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT document_id) as unique_documents,\n",
    "            MIN(evaluation_date) as earliest_date,\n",
    "            MAX(evaluation_date) as latest_date\n",
    "        FROM {REPORTING_DATABASE}.{table}\n",
    "        \"\"\"\n",
    "        \n",
    "        summary_df = execute_athena_query(count_query)\n",
    "        if summary_df is not None and not summary_df.empty:\n",
    "            print(\"\\nSummary:\")\n",
    "            display(summary_df)\n",
    "        else:\n",
    "            print(\"No data found in table\")\n",
    "    else:\n",
    "        print(f\"❌ Could not retrieve schema for {table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_clause = build_where_clause()\n",
    "print(f\"🔧 Active Filters:\")\n",
    "print(f\"  • Time range: {START_DATE or 'No start date'} to {END_DATE or 'No end date'}\")\n",
    "print(f\"  • Document filter: {DOCUMENT_NAME_FILTER or 'None'}\")\n",
    "print(f\"  • SQL WHERE clause: {where_clause or 'No filters applied'}\")\n",
    "\n",
    "print(\"\\n📥 Loading evaluation data...\")\n",
    "\n",
    "# Document-level data\n",
    "doc_query = f\"\"\"\n",
    "SELECT \n",
    "    document_id, input_key, evaluation_date, accuracy, precision, recall, f1_score,\n",
    "    false_alarm_rate, false_discovery_rate, execution_time\n",
    "FROM {REPORTING_DATABASE}.{DOCUMENT_TABLE}\n",
    "{where_clause}\n",
    "ORDER BY evaluation_date DESC\n",
    "\"\"\"\n",
    "\n",
    "doc_df = execute_athena_query(doc_query)\n",
    "if doc_df is not None and not doc_df.empty:\n",
    "    doc_df['evaluation_date'] = pd.to_datetime(doc_df['evaluation_date'])\n",
    "    print(f\"✅ Loaded {len(doc_df)} document evaluation records\")\n",
    "else:\n",
    "    doc_df = pd.DataFrame()\n",
    "    print(\"❌ No document evaluation data found\")\n",
    "\n",
    "# Section-level data\n",
    "section_query = f\"\"\"\n",
    "SELECT \n",
    "    document_id, section_id, section_type, evaluation_date, accuracy, precision, recall, f1_score\n",
    "FROM {REPORTING_DATABASE}.{SECTION_TABLE}\n",
    "{where_clause}\n",
    "ORDER BY evaluation_date DESC\n",
    "\"\"\"\n",
    "\n",
    "section_df = execute_athena_query(section_query)\n",
    "if section_df is not None and not section_df.empty:\n",
    "    section_df['evaluation_date'] = pd.to_datetime(section_df['evaluation_date'])\n",
    "    print(f\"✅ Loaded {len(section_df)} section evaluation records\")\n",
    "else:\n",
    "    section_df = pd.DataFrame()\n",
    "    print(\"❌ No section evaluation data found\")\n",
    "\n",
    "# Attribute-level data\n",
    "attr_query = f\"\"\"\n",
    "SELECT \n",
    "    document_id, attribute_name, expected, actual, matched, score, reason, evaluation_method, evaluation_date\n",
    "FROM {REPORTING_DATABASE}.{ATTRIBUTE_TABLE}\n",
    "{where_clause}\n",
    "ORDER BY evaluation_date DESC\n",
    "\"\"\"\n",
    "\n",
    "attr_df = execute_athena_query(attr_query)\n",
    "if attr_df is not None and not attr_df.empty:\n",
    "    attr_df['evaluation_date'] = pd.to_datetime(attr_df['evaluation_date'])\n",
    "    print(f\"✅ Loaded {len(attr_df)} attribute evaluation records\")\n",
    "else:\n",
    "    attr_df = pd.DataFrame()\n",
    "    print(\"❌ No attribute evaluation data found\")\n",
    "\n",
    "print(\"\\n📊 Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not doc_df.empty:\n",
    "    print(\"📊 Document-Level Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"📈 Document-Level Accuracy Statistics:\")\n",
    "    display(doc_df[['accuracy', 'precision', 'recall', 'f1_score']].describe())\n",
    "    \n",
    "    low_accuracy_docs = doc_df[doc_df['accuracy'] < LOW_ACCURACY_THRESHOLD]\n",
    "    \n",
    "    print(f\"\\n🚨 Documents with Low Accuracy (< {LOW_ACCURACY_THRESHOLD}): {len(low_accuracy_docs)}\")\n",
    "    if not low_accuracy_docs.empty:\n",
    "        display(low_accuracy_docs.sort_values('accuracy')[[\n",
    "            'document_id', 'evaluation_date', 'accuracy', 'precision', 'recall', 'f1_score'\n",
    "        ]].head(TOP_N_ITEMS))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Accuracy distribution\n",
    "    axes[0,0].hist(doc_df['accuracy'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].axvline(LOW_ACCURACY_THRESHOLD, color='red', linestyle='--', label=f'Low ({LOW_ACCURACY_THRESHOLD})')\n",
    "    axes[0,0].axvline(HIGH_ACCURACY_THRESHOLD, color='green', linestyle='--', label=f'High ({HIGH_ACCURACY_THRESHOLD})')\n",
    "    axes[0,0].set_xlabel('Accuracy Score')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].set_title('Document Accuracy Distribution')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Metrics comparison\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    axes[0,1].boxplot([doc_df[m].dropna() for m in metrics], labels=metrics)\n",
    "    axes[0,1].set_title('Evaluation Metrics Distribution')\n",
    "    axes[0,1].set_ylabel('Score')\n",
    "    \n",
    "    # Accuracy over time\n",
    "    if len(doc_df) > 1:\n",
    "        doc_sorted = doc_df.sort_values('evaluation_date')\n",
    "        axes[1,0].scatter(doc_sorted['evaluation_date'], doc_sorted['accuracy'], alpha=0.6)\n",
    "        axes[1,0].axhline(LOW_ACCURACY_THRESHOLD, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1,0].set_xlabel('Evaluation Date')\n",
    "        axes[1,0].set_ylabel('Accuracy')\n",
    "        axes[1,0].set_title('Accuracy Trends Over Time')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Problem vs good documents\n",
    "    good_docs = len(doc_df[doc_df['accuracy'] >= HIGH_ACCURACY_THRESHOLD])\n",
    "    medium_docs = len(doc_df[(doc_df['accuracy'] >= LOW_ACCURACY_THRESHOLD) & (doc_df['accuracy'] < HIGH_ACCURACY_THRESHOLD)])\n",
    "    bad_docs = len(low_accuracy_docs)\n",
    "    \n",
    "    if good_docs + medium_docs + bad_docs > 0:\n",
    "        labels = ['High Accuracy', 'Medium Accuracy', 'Low Accuracy']\n",
    "        sizes = [good_docs, medium_docs, bad_docs]\n",
    "        colors = ['green', 'orange', 'red']\n",
    "        \n",
    "        axes[1,1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1,1].set_title('Document Quality Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ No document data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not section_df.empty:\n",
    "    print(\"📊 Section-Level Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"📈 Section-Level Accuracy Statistics:\")\n",
    "    display(section_df[['accuracy', 'precision', 'recall', 'f1_score']].describe())\n",
    "    \n",
    "    low_accuracy_sections = section_df[section_df['accuracy'] < LOW_ACCURACY_THRESHOLD]\n",
    "    \n",
    "    print(f\"\\n🚨 Sections with Low Accuracy (< {LOW_ACCURACY_THRESHOLD}): {len(low_accuracy_sections)}\")\n",
    "    if not low_accuracy_sections.empty:\n",
    "        display(low_accuracy_sections.sort_values('accuracy')[[\n",
    "            'document_id', 'section_id', 'section_type', 'accuracy', 'f1_score'\n",
    "        ]].head(TOP_N_ITEMS))\n",
    "    \n",
    "    if 'section_type' in section_df.columns:\n",
    "        print(\"\\n📊 Accuracy by Section Type:\")\n",
    "        section_stats = section_df.groupby('section_type')['accuracy'].agg(['count', 'mean', 'std', 'min', 'max']).round(3)\n",
    "        display(section_stats)\n",
    "        \n",
    "        if not low_accuracy_sections.empty:\n",
    "            problem_sections = low_accuracy_sections.groupby('section_type').size().sort_values(ascending=False)\n",
    "            print(f\"\\n🔥 Section Types with Most Problems:\")\n",
    "            display(problem_sections.head(TOP_N_ITEMS))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ No section data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not attr_df.empty:\n",
    "    print(\"📊 Attribute-Level Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"📈 Attribute-Level Statistics:\")\n",
    "    if 'matched' in attr_df.columns:\n",
    "        print(f\"Overall match rate: {attr_df['matched'].mean():.2%}\")\n",
    "    if 'score' in attr_df.columns:\n",
    "        print(f\"Average score: {attr_df['score'].mean():.3f}\")\n",
    "        display(attr_df[['score']].describe())\n",
    "    \n",
    "    if 'score' in attr_df.columns:\n",
    "        low_score_attrs = attr_df[attr_df['score'] < LOW_ACCURACY_THRESHOLD]\n",
    "        \n",
    "        print(f\"\\n🚨 Individual Attribute Issues (score < {LOW_ACCURACY_THRESHOLD}): {len(low_score_attrs)}\")\n",
    "        if not low_score_attrs.empty:\n",
    "            cols_to_show = ['document_id', 'attribute_name']\n",
    "            if 'expected' in attr_df.columns:\n",
    "                cols_to_show.append('expected')\n",
    "            if 'actual' in attr_df.columns:\n",
    "                cols_to_show.append('actual')\n",
    "            cols_to_show.extend(['score'])\n",
    "            if 'reason' in attr_df.columns:\n",
    "                cols_to_show.append('reason')\n",
    "            \n",
    "            display(low_score_attrs[cols_to_show].head(TOP_N_ITEMS))\n",
    "    \n",
    "    if 'attribute_name' in attr_df.columns:\n",
    "        print(\"\\n📊 Attribute Performance Summary:\")\n",
    "        \n",
    "        if 'matched' in attr_df.columns:\n",
    "            attr_stats = attr_df.groupby('attribute_name')['matched'].agg(['count', 'sum', 'mean']).round(3)\n",
    "            attr_stats.columns = ['total_evals', 'matches', 'match_rate']\n",
    "            attr_stats = attr_stats.sort_values('match_rate')\n",
    "            \n",
    "            print(\"\\n🏷️ Worst Performing Attributes:\")\n",
    "            display(attr_stats.head(TOP_N_ITEMS))\n",
    "            \n",
    "            print(\"\\n🏆 Best Performing Attributes:\")\n",
    "            display(attr_stats.tail(TOP_N_ITEMS))\n",
    "    \n",
    "    # Evaluation method performance\n",
    "    if 'evaluation_method' in attr_df.columns and 'matched' in attr_df.columns:\n",
    "        method_stats = attr_df.groupby('evaluation_method')['matched'].agg(['count', 'mean']).round(3)\n",
    "        print(f\"\\n📝 Performance by Evaluation Method:\")\n",
    "        display(method_stats)\n",
    "else:\n",
    "    print(\"❌ No attribute data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 EVALUATION ANALYTICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall summary\n",
    "total_docs = len(doc_df) if not doc_df.empty else 0\n",
    "total_sections = len(section_df) if not section_df.empty else 0\n",
    "total_attributes = len(attr_df) if not attr_df.empty else 0\n",
    "\n",
    "print(f\"\\n📊 Data Overview:\")\n",
    "print(f\"  • Total Documents Evaluated: {total_docs}\")\n",
    "print(f\"  • Total Sections Evaluated: {total_sections}\")\n",
    "print(f\"  • Total Attributes Evaluated: {total_attributes}\")\n",
    "\n",
    "if not doc_df.empty:\n",
    "    avg_doc_accuracy = doc_df['accuracy'].mean()\n",
    "    low_acc_count = len(doc_df[doc_df['accuracy'] < LOW_ACCURACY_THRESHOLD])\n",
    "    print(f\"\\n📈 Document Performance:\")\n",
    "    print(f\"  • Average Document Accuracy: {avg_doc_accuracy:.2%}\")\n",
    "    print(f\"  • Documents Below Threshold ({LOW_ACCURACY_THRESHOLD}): {low_acc_count} ({low_acc_count/total_docs:.1%})\")\n",
    "\n",
    "if not attr_df.empty and 'matched' in attr_df.columns:\n",
    "    avg_match_rate = attr_df['matched'].mean()\n",
    "    print(f\"\\n🏷️ Attribute Performance:\")\n",
    "    print(f\"  • Average Attribute Match Rate: {avg_match_rate:.2%}\")\n",
    "\n",
    "print(f\"\\n💡 Recommendations:\")\n",
    "if not doc_df.empty:\n",
    "    if avg_doc_accuracy < LOW_ACCURACY_THRESHOLD:\n",
    "        print(f\"  • CRITICAL: Overall document accuracy ({avg_doc_accuracy:.1%}) is below threshold\")\n",
    "        print(f\"  • Review model configuration and training data\")\n",
    "    elif low_acc_count > 0:\n",
    "        print(f\"  • Focus on improving {low_acc_count} low-performing documents\")\n",
    "        print(f\"  • Analyze common patterns in failed documents\")\n",
    "    else:\n",
    "        print(f\"  • ✅ Document accuracy performance is good\")\n",
    "\n",
    "if not attr_df.empty and 'matched' in attr_df.columns:\n",
    "    if avg_match_rate < LOW_ACCURACY_THRESHOLD:\n",
    "        print(f\"  • Review attribute extraction logic and prompts\")\n",
    "        print(f\"  • Consider improving evaluation methods\")\n",
    "\n",
    "print(f\"\\n🔍 Next Steps:\")\n",
    "print(f\"  • Use filters in configuration section to focus on specific documents\")\n",
    "print(f\"  • Analyze temporal trends to identify recent performance changes\")\n",
    "print(f\"  • Export problematic cases for detailed manual review\")\n",
    "print(f\"  • Set up regular monitoring using these analytics\")\n",
    "\n",
    "print(f\"\\n✅ Analytics Complete - Review the detailed analysis above!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
