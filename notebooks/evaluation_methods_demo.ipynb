{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Methods Demonstration\n",
    "\n",
    "This notebook demonstrates the various evaluation methods available in the IDP library for comparing expected values with actual extraction results. It covers:\n",
    "\n",
    "1. All evaluation methods with both match and no-match scenarios\n",
    "2. Threshold testing for applicable methods\n",
    "3. Edge cases:\n",
    "   - Attribute not found in actual results\n",
    "   - Attribute not found in expected results\n",
    "   - Attribute not found in either actual or expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: idp_common 0.3.1\n",
      "Uninstalling idp_common-0.3.1:\n",
      "  Successfully uninstalled idp_common-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"../lib/idp_common_pkg[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Add parent directory to path to import the library\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.evaluation.models import EvaluationMethod\n",
    "from idp_common.evaluation.comparator import compare_values\n",
    "from idp_common.evaluation.service import EvaluationService\n",
    "from idp_common.models import Document, Section, Status\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Comparing Individual Values with Different Methods\n",
    "\n",
    "We'll test each evaluation method with matching and non-matching examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comparison(method: EvaluationMethod, expected: Any, actual: Any, \n",
    "                    threshold: float = 0.8, document_class: str = \"TestDoc\",\n",
    "                    attr_name: str = \"test_attr\", attr_description: str = \"Test attribute\"):\n",
    "    \"\"\"Test a comparison method and print results.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Method: {method.name}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    \n",
    "    if method in [EvaluationMethod.FUZZY, EvaluationMethod.BERT]:\n",
    "        print(f\"Threshold: {threshold}\")\n",
    "    \n",
    "    # Set up LLM config for the LLM method\n",
    "    llm_config = None\n",
    "    if method == EvaluationMethod.LLM:\n",
    "        llm_config = {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator that helps determine if the predicted and expected values match for document attribute extraction.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.\"\"\"\n",
    "        }\n",
    "    \n",
    "    # Perform the comparison\n",
    "    matched, score, reason = compare_values(\n",
    "        expected=expected,\n",
    "        actual=actual,\n",
    "        method=method,\n",
    "        threshold=threshold,\n",
    "        document_class=document_class,\n",
    "        attr_name=attr_name,\n",
    "        attr_description=attr_description,\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "    \n",
    "    print(f\"Matched: {matched}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    if reason:\n",
    "        print(f\"Reason: {reason}\")\n",
    "        \n",
    "    return matched, score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: EXACT Method\n",
    "Testing exact string matching with both match and non-match cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account #12345\n",
      "Actual: Account #12345\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account #12345\n",
      "Actual: Account #12346\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account Number: 12345\n",
      "Actual: account number 12345\n",
      "Matched: True\n",
      "Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXACT method - Match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12345\")\n",
    "\n",
    "# EXACT method - No match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12346\")\n",
    "\n",
    "# EXACT method - Match with different casing and punctuation\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account Number: 12345\", \"account number 12345\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: NUMERIC_EXACT Method\n",
    "Testing numeric comparison with different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: $1,250.00\n",
      "Actual: 1250\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: $1,250.00\n",
      "Actual: 1251\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: (1,250.00)\n",
      "Actual: -1250\n",
      "Matched: False\n",
      "Score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.0, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NUMERIC_EXACT method - Match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1250)\n",
    "\n",
    "# NUMERIC_EXACT method - No match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1251)\n",
    "\n",
    "# NUMERIC_EXACT method - Match with different formats\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"(1,250.00)\", \"-1250\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: FUZZY Method\n",
    "Testing fuzzy comparison with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John A. Smith\n",
      "Actual: John Smith\n",
      "Threshold: 0.8\n",
      "Matched: True\n",
      "Score: 0.8333333333333334\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John A. Smith\n",
      "Actual: John Simpson\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.41666666666666663\n",
      "With threshold=0.6: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John Alexander Smith\n",
      "Actual: Jane Marie Johnson\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.15000000000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.15000000000000002, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUZZY method - High match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Smith\", threshold=0.8)\n",
    "\n",
    "# FUZZY method - Medium match \n",
    "matched, score, _ = test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Simpson\", threshold=0.8)\n",
    "print(f\"With threshold=0.6: {score >= 0.6}\")\n",
    "\n",
    "# FUZZY method - Low match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John Alexander Smith\", \"Jane Marie Johnson\", threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: HUNGARIAN Method\n",
    "Testing list comparison using the Hungarian algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "Actual: ['Deposit: $500', 'Transfer: $200', 'Withdrawal: $150']\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "Actual: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $210']\n",
      "Matched: False\n",
      "Score: 0.6666666666666666\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "Actual: ['Deposit: $500', 'Withdrawal: $150']\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: Single item\n",
      "Actual: Single item\n",
      "Matched: True\n",
      "Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HUNGARIAN method - Full match\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Transfer: $200\", \"Withdrawal: $150\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Partial match\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $210\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Different number of items\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Withdrawal: $150\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Non-list values (should convert to list)\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, \"Single item\", \"Single item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: LLM Method\n",
    "Testing semantic comparison using a Large Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '09bdb26a-b77e-431d-9f01-56bd4035b735', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:06 GMT', 'content-type': 'application/json', 'content-length': '410', 'connection': 'keep-alive', 'x-amzn-requestid': '09bdb26a-b77e-431d-9f01-56bd4035b735'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same information about deposits, withdrawals, and the resulting balance, despite slight differences in phrasing and structure.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 204, 'outputTokens': 51, 'totalTokens': 255}, 'metrics': {'latencyMs': 541}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for statement_summary (from code block): match=True, score=0.95, reason=Both values convey the same information about deposits, withdrawals, and the resulting balance, despite slight differences in phrasing and structure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: Both values convey the same information about deposits, withdrawals, and the resulting balance, despite slight differences in phrasing and structure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 0.95,\n",
       " 'Both values convey the same information about deposits, withdrawals, and the resulting balance, despite slight differences in phrasing and structure.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - High semantic match (different wording, same meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Statement with deposits of $2,500 and withdrawals of $1,200, leaving a balance of $3,800.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'b22235c4-1327-4164-8652-84e5d0800a30', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:07 GMT', 'content-type': 'application/json', 'content-length': '429', 'connection': 'keep-alive', 'x-amzn-requestid': 'b22235c4-1327-4164-8652-84e5d0800a30'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.5,\\n  \"reason\": \"The actual value contains different amounts for deposits and withdrawals, resulting in a different ending balance, which does not semantically match the expected value.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 206, 'outputTokens': 53, 'totalTokens': 259}, 'metrics': {'latencyMs': 462}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for statement_summary (from code block): match=False, score=0.5, reason=The actual value contains different amounts for deposits and withdrawals, resulting in a different ending balance, which does not semantically match the expected value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.5\n",
      "Reason: The actual value contains different amounts for deposits and withdrawals, resulting in a different ending balance, which does not semantically match the expected value.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 0.5,\n",
       " 'The actual value contains different amounts for deposits and withdrawals, resulting in a different ending balance, which does not semantically match the expected value.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - No semantic match (different meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits of $2,500 and withdrawals of $1,200, leaving a balance of $3,800.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Policy effective date: January 15, 2023 to January 14, 2024\n",
      "Actual: Policy period begins on Jan 15, 2023 and expires on Jan 15, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '93e18ef0-cd9b-43d5-9604-775e6b62265b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:08 GMT', 'content-type': 'application/json', 'content-length': '429', 'connection': 'keep-alive', 'x-amzn-requestid': '93e18ef0-cd9b-43d5-9604-775e6b62265b'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same period, with minor differences in wording and formatting. The start and end dates are identical, and the meaning is semantically equivalent.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 204, 'outputTokens': 57, 'totalTokens': 261}, 'metrics': {'latencyMs': 486}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for policy_period (from code block): match=True, score=0.95, reason=Both values convey the same period, with minor differences in wording and formatting. The start and end dates are identical, and the meaning is semantically equivalent.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: Both values convey the same period, with minor differences in wording and formatting. The start and end dates are identical, and the meaning is semantically equivalent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 0.95,\n",
       " 'Both values convey the same period, with minor differences in wording and formatting. The start and end dates are identical, and the meaning is semantically equivalent.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - Partial semantic match (some differences)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Policy effective date: January 15, 2023 to January 14, 2024\",\n",
    "    \"Policy period begins on Jan 15, 2023 and expires on Jan 15, 2024\",\n",
    "    document_class=\"InsurancePolicy\",\n",
    "    attr_name=\"policy_period\",\n",
    "    attr_description=\"The dates during which the insurance policy is effective\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Edge Cases - Missing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: Attributes Not Found\n",
    "Testing scenarios where attributes are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Case 1: Attribute not found in actual results\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: This value exists\n",
      "Actual: None\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: This value exists\n",
      "Actual: None\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: NUMERIC_EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: This value exists\n",
      "Actual: None\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: FUZZY - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: This value exists\n",
      "Actual: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'd18799d8-0570-446b-be89-404ddd32db71', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:09 GMT', 'content-type': 'application/json', 'content-length': '386', 'connection': 'keep-alive', 'x-amzn-requestid': 'd18799d8-0570-446b-be89-404ddd32db71'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.0,\\n  \"reason\": \"The expected value indicates that the attribute exists, while the actual value is None, meaning the attribute does not exist.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 149, 'outputTokens': 49, 'totalTokens': 198}, 'metrics': {'latencyMs': 379}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=False, score=0.0, reason=The expected value indicates that the attribute exists, while the actual value is None, meaning the attribute does not exist.\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.0\n",
      "Reason: The expected value indicates that the attribute exists, while the actual value is None, meaning the attribute does not exist.\n",
      "Method: LLM - Score: 0.0 - Matched: False\n",
      "\n",
      "Case 2: Attribute not found in expected results\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: None\n",
      "Actual: This value exists\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: None\n",
      "Actual: This value exists\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: NUMERIC_EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: None\n",
      "Actual: This value exists\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: FUZZY - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: None\n",
      "Actual: This value exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'a241622b-e628-4ade-b064-de6753ba3e96', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:11 GMT', 'content-type': 'application/json', 'content-length': '414', 'connection': 'keep-alive', 'x-amzn-requestid': 'a241622b-e628-4ade-b064-de6753ba3e96'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.1,\\n  \"reason\": \"The expected value is \\'None\\', indicating no value, while the actual value \\'This value exists\\' indicates a value is present. They do not match in meaning.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 149, 'outputTokens': 60, 'totalTokens': 209}, 'metrics': {'latencyMs': 481}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=False, score=0.1, reason=The expected value is 'None', indicating no value, while the actual value 'This value exists' indicates a value is present. They do not match in meaning.\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.1\n",
      "Reason: The expected value is 'None', indicating no value, while the actual value 'This value exists' indicates a value is present. They do not match in meaning.\n",
      "Method: LLM - Score: 0.1 - Matched: False\n",
      "\n",
      "Case 3: Attribute not found in either expected or actual results\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: None\n",
      "Actual: None\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Method: EXACT - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: None\n",
      "Actual: None\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Method: NUMERIC_EXACT - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: None\n",
      "Actual: None\n",
      "Threshold: 0.8\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Method: FUZZY - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: None\n",
      "Actual: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'da6ff544-19d9-4844-8ab3-5d00c59565c2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:12 GMT', 'content-type': 'application/json', 'content-length': '346', 'connection': 'keep-alive', 'x-amzn-requestid': 'da6ff544-19d9-4844-8ab3-5d00c59565c2'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 1.0,\\n  \"reason\": \"Both the expected and actual values are \\'None\\', indicating a perfect match in meaning.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 147, 'outputTokens': 44, 'totalTokens': 191}, 'metrics': {'latencyMs': 666}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=True, score=1.0, reason=Both the expected and actual values are 'None', indicating a perfect match in meaning.\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both the expected and actual values are 'None', indicating a perfect match in meaning.\n",
      "Method: LLM - Score: 1.0 - Matched: True\n",
      "\n",
      "Case 4: Empty string values\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: \n",
      "Actual: \n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Method: EXACT - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: \n",
      "Actual: \n",
      "Threshold: 0.8\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Method: FUZZY - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: \n",
      "Actual: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '25912a7c-55ab-4a52-9925-8a632f7b6cee', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:13 GMT', 'content-type': 'application/json', 'content-length': '358', 'connection': 'keep-alive', 'x-amzn-requestid': '25912a7c-55ab-4a52-9925-8a632f7b6cee'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.5,\\n  \"reason\": \"The expected and actual values do not match in meaning due to significant differences in content.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 145, 'outputTokens': 43, 'totalTokens': 188}, 'metrics': {'latencyMs': 585}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=False, score=0.5, reason=The expected and actual values do not match in meaning due to significant differences in content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.5\n",
      "Reason: The expected and actual values do not match in meaning due to significant differences in content.\n",
      "Method: LLM - Score: 0.5 - Matched: False\n"
     ]
    }
   ],
   "source": [
    "# Case 1: Attribute not found in actual (expected exists, actual is None)\n",
    "print(\"\\nCase 1: Attribute not found in actual results\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.NUMERIC_EXACT, \n",
    "               EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, \"This value exists\", None)\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")\n",
    "\n",
    "# Case 2: Attribute not found in expected (expected is None, actual exists)\n",
    "print(\"\\nCase 2: Attribute not found in expected results\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.NUMERIC_EXACT, \n",
    "               EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, None, \"This value exists\")\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")\n",
    "\n",
    "# Case 3: Attribute not found in either (both are None)\n",
    "print(\"\\nCase 3: Attribute not found in either expected or actual results\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.NUMERIC_EXACT, \n",
    "               EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, None, None)\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")\n",
    "\n",
    "# Case 4: Empty string values (\"\")\n",
    "print(\"\\nCase 4: Empty string values\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, \"\", \"\")\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Full Document Evaluation\n",
    "\n",
    "Now we'll test the full document evaluation service with different evaluation methods. We'll use the real AWS Bedrock service for LLM-based evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test config\n",
    "test_config = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"name\": \"TestDocument\",\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                    \"name\": \"exact_match_attr\",\n",
    "                    \"description\": \"Attribute for exact matching\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"numeric_attr\",\n",
    "                    \"description\": \"Attribute for numeric matching\",\n",
    "                    \"evaluation_method\": \"NUMERIC_EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"fuzzy_attr\",\n",
    "                    \"description\": \"Attribute for fuzzy matching\",\n",
    "                    \"evaluation_method\": \"FUZZY\",\n",
    "                    \"evaluation_threshold\": 0.8\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr\",\n",
    "                    \"description\": \"Attribute for list comparison\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"llm_attr\",\n",
    "                    \"description\": \"Attribute for semantic comparison\",\n",
    "                    \"evaluation_method\": \"LLM\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_actual\",\n",
    "                    \"description\": \"Attribute missing in actual results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_expected\",\n",
    "                    \"description\": \"Attribute missing in expected results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_everywhere\",\n",
    "                    \"description\": \"Attribute missing in both expected and actual\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"evaluation\": {\n",
    "        \"llm_method\": {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator for document extraction attributes.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.\"\"\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock S3 retrieval function\n",
    "def mock_s3_get_json(uri: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mock S3 file retrieval.\"\"\"\n",
    "    if \"expected\" in uri:\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",\n",
    "            \"numeric_attr\": \"$1,250.00\",\n",
    "            \"fuzzy_attr\": \"John Alexander Smith\",\n",
    "            \"list_attr\": [\"Item 1\", \"Item 2\", \"Item 3\"],\n",
    "            \"llm_attr\": \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "            \"missing_in_actual\": \"This value exists in expected only\",\n",
    "            # missing_in_expected is intentionally omitted\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "    else:  # actual results\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",  # Exact match\n",
    "            \"numeric_attr\": 1250,  # Numeric match\n",
    "            \"fuzzy_attr\": \"John A Smith\",  # Fuzzy match\n",
    "            \"list_attr\": [\"Item 1\", \"Item 3\", \"Item 2\"],  # List with different order\n",
    "            \"llm_attr\": \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",  # Semantic match\n",
    "            # missing_in_actual is intentionally omitted\n",
    "            \"missing_in_expected\": \"This value exists in actual only\",\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "\n",
    "# Set up mock storage - we'll still use this for S3\n",
    "class MockS3:\n",
    "    # Store report content for later display\n",
    "    report_content = \"\"\n",
    "    results_content = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_json_content(uri: str) -> Dict[str, Any]:\n",
    "        return mock_s3_get_json(uri)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_content(content: Any, bucket: str, key: str, content_type: str = None):\n",
    "        print(f\"Writing content to s3://{bucket}/{key}\")\n",
    "        if key.endswith(\"results.json\"):\n",
    "            # Store the results for later access\n",
    "            MockS3.results_content = content\n",
    "            print(f\"Evaluation results summary: {json.dumps(content.get('overall_metrics', {}), indent=2)}\")\n",
    "        elif key.endswith(\"report.md\"):\n",
    "            # Store the markdown report for later display\n",
    "            MockS3.report_content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock documents for evaluation\n",
    "def create_test_document(doc_id: str, is_expected: bool = False) -> Document:\n",
    "    \"\"\"Create a test document with a section.\"\"\"\n",
    "    section = Section(\n",
    "        section_id=\"sec-001\",\n",
    "        classification=\"TestDocument\",\n",
    "        extraction_result_uri=f\"s3://test-bucket/{doc_id}/{'expected' if is_expected else 'actual'}/extraction.json\"\n",
    "    )\n",
    "    \n",
    "    doc = Document(\n",
    "        id=doc_id,\n",
    "        sections=[section],\n",
    "        input_key=doc_id,\n",
    "        input_bucket=\"test-bucket\",\n",
    "        output_bucket=\"test-bucket\",\n",
    "        status=Status.EXTRACTED\n",
    "    )\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Create test documents\n",
    "actual_doc = create_test_document(\"test-doc-001\")\n",
    "expected_doc = create_test_document(\"test-doc-001-baseline\", is_expected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.evaluation.service:Initialized evaluation service with LLM configuration\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'e65d6a81-687a-4557-9fa9-d59eab4c425f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 11:59:14 GMT', 'content-type': 'application/json', 'content-length': '414', 'connection': 'keep-alive', 'x-amzn-requestid': 'e65d6a81-687a-4557-9fa9-d59eab4c425f'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same information regarding deposits, withdrawals, and the resulting balance, despite minor differences in phrasing and formatting.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 194, 'outputTokens': 51, 'totalTokens': 245}, 'metrics': {'latencyMs': 469}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for llm_attr (from code block): match=True, score=0.95, reason=Both values convey the same information regarding deposits, withdrawals, and the resulting balance, despite minor differences in phrasing and formatting.\n",
      "INFO:idp_common.evaluation.service:Evaluation complete for document test-doc-001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing content to s3://test-bucket/test-doc-001/evaluation/results.json\n",
      "Evaluation results summary: {\n",
      "  \"precision\": 0.6666666666666666,\n",
      "  \"recall\": 0.8,\n",
      "  \"f1_score\": 0.7272727272727272,\n",
      "  \"accuracy\": 0.625,\n",
      "  \"false_alarm_rate\": 0.5,\n",
      "  \"false_discovery_rate\": 0.2\n",
      "}\n",
      "Writing content to s3://test-bucket/test-doc-001/evaluation/report.md\n",
      "\n",
      "Overall metrics: {'precision': 0.6666666666666666, 'recall': 0.8, 'f1_score': 0.7272727272727272, 'accuracy': 0.625, 'false_alarm_rate': 0.5, 'false_discovery_rate': 0.2}\n",
      "\n",
      "Section sec-001 - Class: TestDocument\n",
      "Metrics: {'precision': 0.6666666666666666, 'recall': 0.8, 'f1_score': 0.7272727272727272, 'accuracy': 0.625, 'false_alarm_rate': 0.5, 'false_discovery_rate': 0.2}\n",
      "\n",
      "Attribute Details:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Name                 Method          Expected                  Actual                    Matched    Score      Reason\n",
      "----------------------------------------------------------------------------------------------------\n",
      "exact_match_attr     EXACT           Exact Match Value         Exact Match Value         True       1.00       \n",
      "numeric_attr         NUMERIC_EXACT   $1,250.00                 1250                      True       1.00       \n",
      "fuzzy_attr           FUZZY           John Alexander Smith      John A Smith              False      0.60       \n",
      "list_attr            HUNGARIAN       ['Item 1', 'Item 2', 'Ite ['Item 1', 'Item 3', 'Ite True       1.00       \n",
      "llm_attr             LLM             Monthly statement showing Statement with deposits t True       0.95       Both values convey the same information regarding ...\n",
      "missing_in_actual    EXACT           This value exists in expe None                      False      0.00       \n",
      "missing_in_expected  EXACT           None                      This value exists in actu False      0.00       \n",
      "missing_everywhere   EXACT           None                      None                      False      1.00       \n"
     ]
    }
   ],
   "source": [
    "# Evaluate document\n",
    "# Only patch S3 module - use real Bedrock\n",
    "import idp_common.evaluation.service\n",
    "idp_common.evaluation.service.s3 = MockS3\n",
    "\n",
    "# Create evaluation service\n",
    "evaluation_service = EvaluationService(region=\"us-east-1\", config=test_config)\n",
    "\n",
    "# Evaluate document\n",
    "result_doc = evaluation_service.evaluate_document(actual_doc, expected_doc, store_results=True)\n",
    "\n",
    "# Print results\n",
    "if hasattr(result_doc, 'evaluation_result'):\n",
    "    eval_result = result_doc.evaluation_result\n",
    "    print(f\"\\nOverall metrics: {eval_result.overall_metrics}\")\n",
    "    \n",
    "    # Check section results\n",
    "    for section_result in eval_result.section_results:\n",
    "        print(f\"\\nSection {section_result.section_id} - Class: {section_result.document_class}\")\n",
    "        print(f\"Metrics: {section_result.metrics}\")\n",
    "        \n",
    "        # Print attribute details\n",
    "        print(\"\\nAttribute Details:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for attr in section_result.attributes:\n",
    "            expected_val = str(attr.expected)[:25]\n",
    "            actual_val = str(attr.actual)[:25]\n",
    "            method = attr.evaluation_method\n",
    "            reason = attr.reason[:50] + \"...\" if attr.reason and len(attr.reason) > 50 else (attr.reason or \"\")\n",
    "            print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Evaluation Report\n",
    "\n",
    "Let's display the markdown evaluation report that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Document Evaluation: test-doc-001\n",
       "\n",
       "## Summary\n",
       "- **Match Rate**: 🟠 4/8 attributes matched [██████████░░░░░░░░░░] 50%\n",
       "- **Precision**: 0.67 | **Recall**: 0.80 | **F1 Score**: 🟡 0.73\n",
       "\n",
       "## Overall Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.6667 | 🟠 Fair |\n",
       "| recall | 0.8000 | 🟡 Good |\n",
       "| f1_score | 0.7273 | 🟡 Good |\n",
       "| accuracy | 0.6250 | 🟠 Fair |\n",
       "| false_alarm_rate | 0.5000 | 🟠 Fair |\n",
       "| false_discovery_rate | 0.2000 | 🟡 Good |\n",
       "\n",
       "\n",
       "## Section: sec-001 (TestDocument)\n",
       "### Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.6667 | 🟠 Fair |\n",
       "| recall | 0.8000 | 🟡 Good |\n",
       "| f1_score | 0.7273 | 🟡 Good |\n",
       "| accuracy | 0.6250 | 🟠 Fair |\n",
       "| false_alarm_rate | 0.5000 | 🟠 Fair |\n",
       "| false_discovery_rate | 0.2000 | 🟡 Good |\n",
       "\n",
       "\n",
       "### Attributes\n",
       "| Status | Attribute | Expected | Actual | Score | Method | Reason |\n",
       "| :----: | --------- | -------- | ------ | ----- | ------ | ------ |\n",
       "| ✅ | exact_match_attr | Exact Match Value | Exact Match Value | 1.00 | EXACT |  |\n",
       "| ✅ | numeric_attr | $1,250.00 | 1250 | 1.00 | NUMERIC_EXACT |  |\n",
       "| ❌ | fuzzy_attr | John Alexander Smith | John A Smith | 0.60 | FUZZY (evaluation_threshold: 0.8) |  |\n",
       "| ✅ | list_attr | ['Item 1', 'Item 2', 'Item 3'] | ['Item 1', 'Item 3', 'Item 2'] | 1.00 | HUNGARIAN |  |\n",
       "| ✅ | llm_attr | Monthly statement showing deposits of $1,250, with | Statement with deposits totaling $1,250 and withdr | 0.95 | LLM | Both values convey the same information regarding deposits, withdrawals, and the |\n",
       "| ❌ | missing_in_actual | This value exists in expected only | None | 0.00 | EXACT |  |\n",
       "| ❌ | missing_in_expected | None | This value exists in actual only | 0.00 | EXACT |  |\n",
       "| ❌ | missing_everywhere | None | None | 1.00 | EXACT |  |\n",
       "\n",
       "\n",
       "Execution time: 1.22 seconds\n",
       "\n",
       "## Evaluation Methods Used\n",
       "\n",
       "This evaluation used the following methods to compare expected and actual values:\n",
       "\n",
       "1. **EXACT** - Exact string match after stripping punctuation and whitespace\n",
       "2. **NUMERIC_EXACT** - Exact numeric match after normalizing\n",
       "3. **FUZZY** - Fuzzy string matching using string similarity metrics (with optional evaluation_threshold)\n",
       "4. **BERT** - Semantic similarity comparison using BERT embeddings (with evaluation_threshold)\n",
       "5. **HUNGARIAN** - Bipartite matching algorithm for lists of values\n",
       "6. **LLM** - Advanced semantic evaluation using Bedrock large language models\n",
       "\n",
       "Each attribute is configured with a specific evaluation method based on the data type and comparison needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Display the markdown report\n",
    "if MockS3.report_content:\n",
    "    display(Markdown(MockS3.report_content))\n",
    "else:\n",
    "    print(\"No evaluation report was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. All evaluation methods available in the IDP library:\n",
    "   - EXACT - Exact string matching\n",
    "   - NUMERIC_EXACT - Numeric value matching\n",
    "   - FUZZY - Fuzzy string matching with adjustable thresholds\n",
    "   - HUNGARIAN - List comparison using the Hungarian algorithm\n",
    "   - LLM - Semantic comparison using Large Language Models\n",
    "\n",
    "2. Handling of edge cases:\n",
    "   - Attributes missing in actual results\n",
    "   - Attributes missing in expected results\n",
    "   - Attributes missing in both actual and expected results\n",
    "   - Empty string values\n",
    "\n",
    "3. Full document evaluation with mixed evaluation methods\n",
    "   - Comprehensive metrics calculation\n",
    "   - Detailed attribute-level results\n",
    "\n",
    "4. Threshold sensitivity analysis for fuzzy matching\n",
    "   - How different threshold values affect match results\n",
    "   - Trade-offs between precision and recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
