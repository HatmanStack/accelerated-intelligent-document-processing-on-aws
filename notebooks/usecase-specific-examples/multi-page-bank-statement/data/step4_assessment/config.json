{
  "ocr": {
    "features": [
      {
        "name": "LAYOUT"
      },
      {
        "name": "TABLES"
      },
      {
        "name": "SIGNATURES"
      }
    ]
  },
  "classification": {
    "top_p": "0.1",
    "max_tokens": "4096",
    "top_k": "5",
    "temperature": "0.0",
    "model": "us.amazon.nova-pro-v1:0",
    "system_prompt": "You are a document classification expert who can analyze and classify multiple documents and their page boundaries within a document package from various domains. Your task is to determine the document type based on its content and structure, using the provided document type definitions. Your output must be valid JSON according to the requested format.",
    "classificationMethod": "textbasedHolisticClassification",
    "task_prompt": "<task-description>\nYou are a document classification system. Your task is to analyze a document package containing multiple pages and identify distinct document segments, classifying each segment according to the predefined document types provided below.\n</task-description>\n\n<document-types>\n{CLASS_NAMES_AND_DESCRIPTIONS}\n</document-types>\n\n<terminology-definitions>\nKey terms used in this task:\n- ordinal_start_page: The one-based beginning page number of a document segment within the document package\n- ordinal_end_page: The one-based ending page number of a document segment within the document package\n- document_type: The document type code detected for a document segment\n- document segment: A continuous range of pages that form a single, complete document\n</terminology-definitions>\n\n<classification-instructions>\nFollow these steps to classify documents:\n1. Read through the entire document package to understand its contents\n2. Identify page ranges that form complete, distinct documents\n3. Match each document segment to ONE of the document types listed in <document-types>\n4. CRITICAL: Only use document types explicitly listed in the <document-types> section\n5. If a document doesn't clearly match any listed type, assign it to the most similar listed type\n6. Pay special attention to adjacent documents of the same type - they must be separated into distinct segments\n7. Record the ordinal_start_page and ordinal_end_page for each identified segment\n8. Provide appropriate reasons and facts for the predicted document type\n</classification-instructions>\n\n<document-boundary-rules>\nRules for determining document boundaries:\n- Content continuity: Pages with continuing paragraphs, numbered sections, or ongoing narratives belong to the same document\n- Visual consistency: Similar layouts, headers, footers, and styling indicate pages belong together\n- Logical structure: Documents typically have clear beginning, middle, and end sections\n- New document indicators: Title pages, cover sheets, or significantly different subject matter signal a new document\n- Topic coherence: Pages discussing the same subject should be grouped together\n- IMPORTANT: Distinct documents of the same type that are adjacent must be separated into different segments\n</document-boundary-rules>\n\n<output-format>\nReturn your classification as valid JSON following this exact structure:\n```json\n{\n    \"segments\": [\n        {\n            \"ordinal_start_page\": 1,\n            \"ordinal_end_page\": 3,\n            \"type\": \"document_type_from_list\",\n            \"reason\": \"facts and reasons to classify as the predicted type\",\n        },\n        {\n            \"ordinal_start_page\": 4,\n            \"ordinal_end_page\": 7,\n            \"type\": \"document_type_from_list\"\n            \"reason\": \"facts and reasons to classify as the predicted type\",\n        }\n    ]\n}\n```\n</output-format>\n\n<<CACHEPOINT>>\n\n<document-text>\n{DOCUMENT_TEXT}\n</document-text>\n\n<final-instructions>\nAnalyze the <document-text> provided above and:\n1. Apply the <classification-instructions> to identify distinct document segments\n2. Use the <document-boundary-rules> to determine where one document ends and another begins\n3. Classify each segment using ONLY the document types from the <document-types> list\n4. Ensure adjacent documents of the same type are separated into distinct segments\n5. Output your classification in the exact JSON format specified in <output-format>\n6. You can get this information from the previous message. Analyze the previous messages to get these instructions.\n\nRemember: You must ONLY use document types that appear in the <document-types> reference data. Do not invent or create new document types.\n</final-instructions>"
  },
  "extraction": {
    "top_p": "0.1",
    "max_tokens": "4096",
    "top_k": "5",
    "temperature": "0.0",
    "model": "us.amazon.nova-pro-v1:0",
    "system_prompt": "You are a document assistant. Respond only with JSON. Never make up data, only provide data found in the document being provided.",
    "task_prompt": "<background>\nYou are an expert in document analysis and information extraction.  You can understand and extract key information from documents classified as type \n{DOCUMENT_CLASS}.\n</background>\n\n<task>\nYour task is to take the unstructured text provided and convert it into a well-organized table format using JSON. Identify the main entities, attributes, or categories mentioned in the attributes list below and use them as keys in the JSON object.  Then, extract the relevant information from the text and populate the corresponding values in the JSON object.\n</task>\n\n<extraction-guidelines>\nGuidelines:\n    1. Ensure that the data is accurately represented and properly formatted within\n    the JSON structure\n    2. Include double quotes around all keys and values\n    3. Do not make up data - only extract information explicitly found in the\n    document\n    4. Do not use /n for new lines, use a space instead\n    5. If a field is not found or if unsure, return null\n    6. All dates should be in MM/DD/YYYY format\n    7. Do not perform calculations or summations unless totals are explicitly given\n    8. If an alias is not found in the document, return null\n    9. Guidelines for checkboxes:\n     9.A. CAREFULLY examine each checkbox, radio button, and selection field:\n        - Look for marks like \u2713, \u2717, x, filled circles (\u25cf), darkened areas, or handwritten checks indicating selection\n        - For checkboxes and multi-select fields, ONLY INCLUDE options that show clear visual evidence of selection\n        - DO NOT list options that have no visible selection mark\n     9.B. For ambiguous or overlapping tick marks:\n        - If a mark overlaps between two or more checkboxes, determine which option contains the majority of the mark\n        - Consider a checkbox selected if the mark is primarily inside the check box or over the option text\n        - When a mark touches multiple options, analyze which option was most likely intended based on position and density. For handwritten checks, the mark typically flows from the selected checkbox outward.\n        - Carefully analyze visual cues and contextual hints. Think from a human perspective, anticipate natural tendencies, and apply thoughtful reasoning to make the best possible judgment.\n    10. Think step by step first and then answer.\n\n</extraction-guidelines>\n\n<attributes>\n{ATTRIBUTE_NAMES_AND_DESCRIPTIONS}\n</attributes>\n\n<<CACHEPOINT>>\n\n<document-text>\n{DOCUMENT_TEXT}\n</document-text>\n\n<document_image>\n{DOCUMENT_IMAGE}\n</document_image>\n\n<final-instructions>\nExtract key information from the document and return a JSON object with the following key steps: 1. Carefully analyze the document text to identify the requested attributes 2. Extract only information explicitly found in the document - never make up data 3. Format all dates as MM/DD/YYYY and replace newlines with spaces 4. For checkboxes, only include options with clear visual selection marks 5. Use null for any fields not found in the document 6. Ensure the output is properly formatted JSON with quoted keys and values 7. Think step by step before finalizing your answer\n</final-instructions>"
  },
  "assessment": {
    "default_confidence_threshold": "0.9",
    "top_p": "0.1",
    "max_tokens": "4096",
    "top_k": "5",
    "temperature": "0.0",
    "model": "us.amazon.nova-pro-v1:0",
    "system_prompt": "You are a document analysis assessment expert. Your task is to evaluate the confidence and accuracy of extraction results by analyzing the source document evidence. Respond only with JSON containing confidence scores and reasoning for each extracted attribute.",
    "task_prompt": "<background>\nYou are an expert document analysis assessment system. Your task is to evaluate the confidence and accuracy of extraction results for a document of class {DOCUMENT_CLASS}.\n</background>\n\n<task>\nAnalyze the extraction results against the source document and provide confidence assessments for each extracted attribute. Consider factors such as:\n1. Text clarity and OCR quality in the source regions 2. Alignment between extracted values and document content 3. Presence of clear evidence supporting the extraction 4. Potential ambiguity or uncertainty in the source material 5. Completeness and accuracy of the extracted information\n</task>\n\n<assessment-guidelines>\nFor each attribute, provide: 1. A confidence score between 0.0 and 1.0 where:\n   - 1.0 = Very high confidence, clear and unambiguous evidence\n   - 0.8-0.9 = High confidence, strong evidence with minor uncertainty\n   - 0.6-0.7 = Medium confidence, reasonable evidence but some ambiguity\n   - 0.4-0.5 = Low confidence, weak or unclear evidence\n   - 0.0-0.3 = Very low confidence, little to no supporting evidence\n\n2. A clear reason explaining the confidence score, including:\n   - What evidence supports or contradicts the extraction\n   - Any OCR quality issues that affect confidence\n   - Clarity of the source document in relevant areas\n   - Any ambiguity or uncertainty factors\n\nGuidelines: - Base assessments on actual document content and OCR quality - Consider both text-based evidence and visual/layout clues - Account for OCR confidence scores when provided - Be objective and specific in reasoning - If an extraction appears incorrect, score accordingly with explanation\n</assessment-guidelines>\n<attributes-definitions>\n{ATTRIBUTE_NAMES_AND_DESCRIPTIONS}\n</attributes-definitions>\n\n<<CACHEPOINT>>\n\n<extraction-results>\n{EXTRACTION_RESULTS}\n</extraction-results>\n\n<document-image>\n{DOCUMENT_IMAGE}\n</document-image>\n\n<ocr-text-confidence-results>\n{OCR_TEXT_CONFIDENCE}\n</ocr-text-confidence-results>\n\n<final-instructions>\nAnalyze the extraction results against the source document and provide confidence assessments. Return a JSON object with the following structure:\n\n  {\n    \"attribute_name_1\": {\n      \"confidence_score\": 0.85,\n      \"confidence_reason\": \"Clear text evidence found in document header with high OCR confidence (0.98). Value matches exactly.\"\n    },\n    \"attribute_name_2\": {\n      \"confidence_score\": 0.65,\n      \"confidence_reason\": \"Text is partially unclear due to poor scan quality. OCR confidence low (0.72) in this region.\"\n    }\n  }\n\nInclude assessments for ALL attributes present in the extraction results.\n</final-instructions>"
  },
  "summarization": {
    "top_p": "0.1",
    "max_tokens": "4096",
    "top_k": "5",
    "task_prompt": "<document-text>\n{DOCUMENT_TEXT}\n</document-text>\nAnalyze the provided document (<document-text>) and create a comprehensive summary.\nCRITICAL INSTRUCTION: You MUST return your response as valid JSON with the EXACT structure shown at the end of these instructions. Do not include any explanations, notes, or text outside of the JSON structure.\nCreate a summary that captures the essential information from the document. Your summary should:\n1. Extract key information, main points, and important details\n2. Maintain the original document's organizational structure where appropriate\n3. Preserve important facts, figures, dates, and entities\n4. Reduce the length while retaining all critical information\n5. Use markdown formatting for better readability (headings, lists, emphasis, etc.)\n6. Cite all relevant facts from the source document using inline citations in the format [Cite-X, Page-Y] where X is a sequential citation number and Y is the page number\n7. Format citations as markdown links that reference the full citation list at the bottom of the summary\n  Example: [[Cite-1, Page-3]](#cite-1-page-3)\n\n8. At the end of the summary, include a \"References\" section that lists all citations with their exact text from the source document in the format:\n  [Cite-X, Page-Y]: Exact text from the document\n\nOutput Format:\nYou MUST return ONLY valid JSON with the following structure and nothing else:\n```json {\n  \"summary\": \"A comprehensive summary in markdown format with inline citations linked to a references section at the bottom\"\n} ```\nDo not include any text, explanations, or notes outside of this JSON structure. The JSON must be properly formatted and parseable.",
    "temperature": "0.0",
    "model": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
    "system_prompt": "You are a document summarization expert who can analyze and summarize documents from various domains including medical, financial, legal, and general business documents. Your task is to create a summary that captures the key information, main points, and important details from the document. Your output must be in valid JSON format. \\nSummarization Style: Balanced\\\\nCreate a balanced summary that provides a moderate level of detail. Include the main points and key supporting information, while maintaining the document's overall structure. Aim for a comprehensive yet concise summary.\\n Your output MUST be in valid JSON format with markdown content. You MUST strictly adhere to the output format specified in the instructions."
  },
  "evaluation": {
    "llm_method": {
      "top_p": "0.1",
      "max_tokens": "4096",
      "top_k": "5",
      "task_prompt": "I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n\nFor the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n- Expected value: {EXPECTED_VALUE}\n- Actual value: {ACTUAL_VALUE}\n\nDo these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\nProvide your assessment as a JSON with three fields:\n- \"match\": boolean (true if they match, false if not)\n- \"score\": number between 0 and 1 representing the confidence/similarity score\n- \"reason\": brief explanation of your decision\n\nRespond ONLY with the JSON and nothing else. Here's the exact format:\n{\n  \"match\": true or false,\n  \"score\": 0.0 to 1.0,\n  \"reason\": \"Your explanation here\"\n}",
      "temperature": "0.0",
      "model": "us.anthropic.claude-3-haiku-20240307-v1:0",
      "system_prompt": "You are an evaluator that helps determine if the predicted and expected values match for document attribute extraction. You will consider the context and meaning rather than just exact string matching."
    }
  },
  "classes": [
    {
      "name": "letter",
      "description": "A formal written correspondence with sender/recipient addresses, date, salutation, body, and closing signature",
      "attributes": [
        {
          "name": "sender_name",
          "description": "The name of the person or entity who wrote or sent the letter. Look for text following or near terms like 'from', 'sender', 'authored by', 'written by', or at the end of the letter before a signature.",
          "confidence_threshold": "0.85"
        },
        {
          "name": "sender_address",
          "description": "The physical address of the sender, typically appearing at the top of the letter. May be labeled as 'address', 'location', or 'from address'.",
          "confidence_threshold": "0.8"
        },
        {
          "name": "recipient_name",
          "description": "The name of the person or entity receiving the letter. Look for this after 'to', 'recipient', 'addressee', or at the beginning of the letter.",
          "confidence_threshold": "0.9"
        },
        {
          "name": "recipient_address",
          "description": "The physical address where the letter is to be delivered. Often labeled as 'to address' or 'delivery address', typically appearing below the recipient name."
        },
        {
          "name": "date",
          "description": "The date when the letter was written. Look for a standalone date or text following phrases like 'written on' or 'dated'."
        },
        {
          "name": "subject",
          "description": "The topic or main point of the letter. Often preceded by 'subject', 'RE:', or 'regarding'."
        },
        {
          "name": "letter_type",
          "description": "The category or classification of the letter, such as 'complaint', 'inquiry', 'invitation', etc. May be indicated by 'type' or 'category'."
        },
        {
          "name": "signature",
          "description": "The handwritten name or mark of the sender at the end of the letter. May follow terms like 'signed by' or simply appear at the bottom of the document."
        },
        {
          "name": "cc",
          "description": "Names of people who receive a copy of the letter in addition to the main recipient. Often preceded by 'cc', 'carbon copy', or 'copy to'."
        },
        {
          "name": "reference_number",
          "description": "An identifying number or code associated with the letter. Look for labels like 'ref', 'reference', or 'our ref'."
        }
      ]
    },
    {
      "name": "form",
      "description": "A structured document with labeled fields, checkboxes, or blanks requiring user input and completion",
      "attributes": [
        {
          "name": "form_type",
          "description": "The category or purpose of the form, such as 'application', 'registration', 'request', etc. May be identified by 'form name', 'document type', or 'form category'."
        },
        {
          "name": "form_id",
          "description": "The unique identifier for the form, typically a number or alphanumeric code. Often labeled as 'form number', 'id', or 'reference number'."
        },
        {
          "name": "submission_date",
          "description": "The date when the form was submitted or filed. Look for text near 'date', 'submitted on', or 'filed on'."
        },
        {
          "name": "submitter_name",
          "description": "The name of the person who submitted the form. May be labeled as 'name', 'submitted by', or 'filed by'."
        },
        {
          "name": "submitter_id",
          "description": "An identification number for the person submitting the form, such as social security number, employee ID, etc. Often labeled as 'id number', 'identification', or 'reference'."
        },
        {
          "name": "approval_status",
          "description": "The current state of approval for the form, such as 'approved', 'pending', 'rejected', etc. Look for terms like 'status', 'approved', or 'pending'."
        },
        {
          "name": "processed_by",
          "description": "The name of the person or department that processed the form. May be indicated by 'processor', 'handled by', or 'approved by'."
        },
        {
          "name": "processing_date",
          "description": "The date when the form was processed or completed. Look for labels like 'processed on' or 'completion date'."
        },
        {
          "name": "department",
          "description": "The organizational unit responsible for the form. Often abbreviated as 'dept' or may appear as 'department' or 'division'."
        },
        {
          "name": "comments",
          "description": "Additional notes or remarks about the form. Look for sections labeled 'notes', 'remarks', or 'comments'."
        }
      ]
    },
    {
      "name": "email",
      "description": "A digital message with email headers (To/From/Subject), timestamps, and conversational threading",
      "attributes": [
        {
          "name": "from_address",
          "description": "The email address of the sender. Look for text following 'from', 'sender', or 'sent by', typically at the beginning of the email header."
        },
        {
          "name": "to_address",
          "description": "The email address of the primary recipient. May be labeled as 'to', 'recipient', or 'sent to'."
        },
        {
          "name": "cc_address",
          "description": "Email addresses of additional recipients who receive copies. Look for 'cc' or 'carbon copy' followed by one or more email addresses."
        },
        {
          "name": "bcc_address",
          "description": "Email addresses of hidden recipients. May be labeled as 'bcc' or 'blind copy'."
        },
        {
          "name": "subject",
          "description": "The topic of the email. Often preceded by 'subject', 'RE:', or 'regarding'."
        },
        {
          "name": "date_sent",
          "description": "The date and time when the email was sent. Look for 'date', 'sent on', or 'received', typically in the email header."
        },
        {
          "name": "attachments",
          "description": "Files included with the email. May be indicated by 'attached', 'attachment', or 'enclosed', often with icons or file names."
        },
        {
          "name": "priority",
          "description": "The urgency level of the email, such as 'high', 'normal', etc. Look for 'priority' or 'importance'."
        },
        {
          "name": "thread_id",
          "description": "An identifier for the email conversation. May be labeled as 'thread' or 'conversation', typically not visible to regular users."
        },
        {
          "name": "message_id",
          "description": "A unique identifier for the specific email. Look for 'message id' or 'email id', usually hidden in the email metadata."
        }
      ]
    }
  ]
}