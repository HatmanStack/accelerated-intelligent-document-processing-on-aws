{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holistic Packet Classification with IDP Common Package\n",
    "\n",
    "This notebook demonstrates how to use the holistic packet classification capability of the IDP Common Package to classify multi-document packets, where each document might span multiple pages. The holistic approach examines the document as a whole to identify boundaries between different document types within the packet.\n",
    "\n",
    "**Key Benefits of Holistic Packet Classification:**\n",
    "1. Properly handles multi-page documents within a packet\n",
    "2. Detects logical document boundaries\n",
    "3. Identifies document types in context of the whole document\n",
    "4. Handles documents where individual pages may not be clearly classifiable on their own\n",
    "\n",
    "> **Note**: This notebook uses real AWS services including S3, Textract, and Bedrock. You need valid AWS credentials with appropriate permissions to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Let's install the IDP common package in development mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: idp_common 0.3.0\n",
      "Uninstalling idp_common-0.3.0:\n",
      "  Successfully uninstalled idp_common-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Version: 0.3.0\n",
      "Location: /home/ec2-user/miniconda/lib/python3.12/site-packages\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"../lib/idp_common_pkg[all]\"\n",
    "\n",
    "# Check installed version\n",
    "%pip show idp_common | grep -E \"Version|Location\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup:\n",
      "METRIC_NAMESPACE: IDP-Holistic-Classification-Example\n",
      "AWS_REGION: us-west-2\n",
      "Input bucket: idp-holistic-input-912625584728-us-west-2\n",
      "Output bucket: idp-holistic-output-912625584728-us-west-2\n",
      "SAMPLE_PDF_PATH: ../samples/rvl_cdip_package.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import sys\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configure logging - target only the OCR and classification service modules\n",
    "logging.basicConfig(level=logging.WARNING)  # Set root logger to WARNING (less verbose)\n",
    "logging.getLogger('idp_common.ocr.service').setLevel(logging.INFO)\n",
    "logging.getLogger('idp_common.classification.service').setLevel(logging.INFO)\n",
    "logging.getLogger('textractor').setLevel(logging.WARNING)  # Suppress textractor logs\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Holistic-Classification-Example'\n",
    "os.environ['AWS_REGION'] = boto3.session.Session().region_name or 'us-east-1'\n",
    "os.environ['CONFIGURATION_TABLE_NAME'] = 'mock-config-table'\n",
    "\n",
    "# Get AWS account ID for unique bucket names\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = os.environ['AWS_REGION']\n",
    "\n",
    "# Define sample PDF path \n",
    "SAMPLE_PDF_PATH = \"../samples/rvl_cdip_package.pdf\"\n",
    "\n",
    "# Create unique bucket names based on account ID and region\n",
    "input_bucket_name = f\"idp-holistic-input-{account_id}-{region}\"\n",
    "output_bucket_name = f\"idp-holistic-output-{account_id}-{region}\"\n",
    "\n",
    "# Import base libraries\n",
    "from idp_common.models import Document, Status\n",
    "from idp_common import ocr, classification, extraction, get_config, utils\n",
    "\n",
    "# Helper function to parse S3 URIs\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "# Helper function to load JSON from S3\n",
    "def load_json_from_s3(uri):\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Environment setup:\")\n",
    "print(f\"METRIC_NAMESPACE: {os.environ.get('METRIC_NAMESPACE')}\")\n",
    "print(f\"AWS_REGION: {os.environ.get('AWS_REGION')}\")\n",
    "print(f\"Input bucket: {input_bucket_name}\")\n",
    "print(f\"Output bucket: {output_bucket_name}\")\n",
    "print(f\"SAMPLE_PDF_PATH: {SAMPLE_PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up S3 Buckets and Upload Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket idp-holistic-input-912625584728-us-west-2 already exists\n",
      "Bucket idp-holistic-output-912625584728-us-west-2 already exists\n",
      "Uploaded sample file to: s3://idp-holistic-input-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Function to create a bucket if it doesn't exist\n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=bucket_name)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f\"Created bucket: {bucket_name}\")\n",
    "            \n",
    "            # Wait for bucket to be accessible\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(Bucket=bucket_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Ensure both buckets exist\n",
    "ensure_bucket_exists(input_bucket_name)\n",
    "ensure_bucket_exists(output_bucket_name)\n",
    "\n",
    "# Upload the sample file to S3\n",
    "sample_file_key = \"sample-\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".pdf\"\n",
    "try:\n",
    "    with open(SAMPLE_PDF_PATH, 'rb') as file_data:\n",
    "        s3_client.upload_fileobj(file_data, input_bucket_name, sample_file_key)\n",
    "    print(f\"Uploaded sample file to: s3://{input_bucket_name}/{sample_file_key}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Sample file not found at {SAMPLE_PDF_PATH}. Please ensure the sample file exists.\")\n",
    "    # Use a default sample if available\n",
    "    alt_sample_path = \"../samples/sample.pdf\"\n",
    "    try:\n",
    "        with open(alt_sample_path, 'rb') as file_data:\n",
    "            s3_client.upload_fileobj(file_data, input_bucket_name, sample_file_key)\n",
    "        print(f\"Used alternative sample file: s3://{input_bucket_name}/{sample_file_key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No sample files found. Please create a samples directory with PDF files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test configuration created for IDP services with textbased holistic classification method enabled\n"
     ]
    }
   ],
   "source": [
    "# Sample configuration that mimics what would be in DynamoDB\n",
    "CONFIG = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "        \"name\": \"letter\",\n",
    "        \"description\": \"A formal written message that is typically sent from one person to another\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"sender_name\",\n",
    "            \"description\": \"The name of the person or entity who wrote or sent the letter. Look for text following or near terms like 'from', 'sender', 'authored by', 'written by', or at the end of the letter before a signature.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"sender_address\",\n",
    "            \"description\": \"The physical address of the sender, typically appearing at the top of the letter. May be labeled as 'address', 'location', or 'from address'.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"form\",\n",
    "        \"description\": \"A document with blank spaces for filling in information\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"form_type\",\n",
    "            \"description\": \"The category or purpose of the form, such as 'application', 'registration', 'request', etc. May be identified by 'form name', 'document type', or 'form category'.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"form_id\",\n",
    "            \"description\": \"The unique identifier for the form, typically a number or alphanumeric code. Often labeled as 'form number', 'id', or 'reference number'.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"invoice\",\n",
    "        \"description\": \"A commercial document issued by a seller to a buyer relating to a sale\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"invoice_number\",\n",
    "            \"description\": \"The unique identifier for the invoice. Look for 'invoice no', 'invoice #', or 'bill number', typically near the top of the document.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"invoice_date\",\n",
    "            \"description\": \"The date when the invoice was issued. May be labeled as 'date', 'invoice date', or 'billing date'.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"resume\",\n",
    "        \"description\": \"A document summarizing a person's background, skills, and qualifications\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"full_name\",\n",
    "            \"description\": \"The complete name of the job applicant, typically appearing prominently at the top of the resume. May be simply labeled as 'name' or 'applicant name'.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"contact_info\",\n",
    "            \"description\": \"The phone number, email, and address of the applicant. Look for a section with 'contact', 'phone', 'email', or 'address', usually near the top of the resume.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"scientific_publication\",\n",
    "        \"description\": \"A formally published document presenting scientific research findings\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"title\",\n",
    "            \"description\": \"The name of the scientific paper, typically appearing prominently at the beginning. May be labeled as 'title', 'paper title', or 'article title'.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"authors\",\n",
    "            \"description\": \"The researchers who conducted the study and wrote the paper. Look for names after 'authors', 'contributors', or 'researchers', usually following the title.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"memo\",\n",
    "        \"description\": \"A brief written message used for internal communication within an organization\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"memo_date\",\n",
    "            \"description\": \"The date when the memo was written. Look for 'date' or 'memo date', typically near the top of the document.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"from\",\n",
    "            \"description\": \"The person or department that wrote the memo. May be labeled as 'from', 'sender', or 'author'.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"advertisement\",\n",
    "        \"description\": \"A public notice promoting a product, service, or event\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"product_name\",\n",
    "            \"description\": \"The name of the item or service being advertised. Look for prominently displayed text that could be a 'product', 'item', or 'service' name.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"brand\",\n",
    "            \"description\": \"The company or manufacturer of the product. May be indicated by a logo or text labeled as 'brand', 'company', or 'manufacturer'.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"email\",\n",
    "        \"description\": \"An electronic message sent from one person to another over a computer network\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"from_address\",\n",
    "            \"description\": \"The email address of the sender. Look for text following 'from', 'sender', or 'sent by', typically at the beginning of the email header.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"to_address\",\n",
    "            \"description\": \"The email address of the primary recipient. May be labeled as 'to', 'recipient', or 'sent to'.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"questionnaire\",\n",
    "        \"description\": \"A set of written questions designed to collect information from respondents\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"form_title\",\n",
    "            \"description\": \"The name or title of the questionnaire. Look for prominently displayed text at the beginning that could be a 'title', 'survey name', or 'questionnaire name'.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"respondent_info\",\n",
    "            \"description\": \"Information about the person completing the questionnaire. May include fields labeled 'respondent', 'participant', or 'name'.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"specification\",\n",
    "        \"description\": \"A detailed description of technical requirements or characteristics\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"product_name\",\n",
    "            \"description\": \"The name of the item being specified. Look for text labeled as 'product', 'item', or 'model', typically appearing prominently at the beginning.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"version\",\n",
    "            \"description\": \"The iteration or release number. May be indicated by 'version', 'revision', or 'release', often followed by a number or code.\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"generic\",\n",
    "        \"description\": \"A general document type that doesn't fit into other specific categories\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"document_type\",\n",
    "            \"description\": \"The classification or category of the document. Look for terms like 'type', 'category', or 'class' that indicate what kind of document this is.\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"document_date\",\n",
    "            \"description\": \"The date when the document was created. May be labeled as 'date', 'created on', or 'issued on'.\"\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    ],\n",
    "  \"classification\": {\n",
    "    \"temperature\": \"0\",\n",
    "    \"model\": \"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"classificationMethod\": \"textbasedHolisticClassification\",  # Use holistic packet classification\n",
    "    \"system_prompt\": \"You are a document classification expert who can analyze and classify multiple documents and their page boundaries within a document package from various domains. Your task is to determine the document type based on its content and structure, using the provided document type definitions. Your output must be valid JSON according to the requested format.\",\n",
    "    \"top_k\": \"200\",\n",
    "    \"task_prompt\": \"\"\"The <document-text> XML tags contains the text separated into pages from the document package. Each page will begin with a <page-number> XML tag indicating the one based page ordinal of the page text to follow.\n",
    "<document-text>\n",
    "{DOCUMENT_TEXT}\n",
    "</document-text>\n",
    "\n",
    "The <document-types> XML tags contain a markdown table of known doc types for detection.\n",
    "<document-types>\n",
    "{CLASS_NAMES_AND_DESCRIPTIONS}\n",
    "</document-types>\n",
    "\n",
    "<guidance>\n",
    "Guidance for terminology found in the instructions.\n",
    "    * ordinal_start_page: The one based beginning page of a document segment within the document package.\n",
    "    * ordinal_end_page: The one based ending page of a document segment within the document package.\n",
    "    * document_type: The document type code detected for a document segment.\n",
    "    * Distinct documents of the same type may be adjacent to each other in the packet. Be sure to separate them into different document segments and don't combine them.\n",
    "</guidance>\n",
    "\n",
    "Follow these steps when classifying documents within the document package:\n",
    "1. Examine the document package as a whole, and identify page ranges that are likely to belong to one of the <document-types>.\n",
    "2. Match each page range with an identified document type.\n",
    "3. Identify documents of the same type, that are not the same document but are adjacent to each other in the packet.\n",
    "4. Separate unique documents of the same type adjacent to each other in the packet into distinct document segments. Important: Do not combine distinct documents of the same type into a single document segment.\n",
    "5. For each identified document type, note the ordinal_start_page and ordinal_end_page.\n",
    "6. Compile the classified documents into a list with their respective ordinal_start_page and ordinal_end_page.\n",
    "\n",
    "Return your response as valid JSON according to this format:\n",
    "```json\n",
    "{\n",
    "    \"segments\": [\n",
    "                      {\n",
    "                        \"ordinal_start_page\": 1,\n",
    "                        \"ordinal_end_page\": 2,\n",
    "                        \"type\": \"the first type of document detected\"\n",
    "                      },\n",
    "                      {\n",
    "                        \"ordinal_start_page\": 3,\n",
    "                        \"ordinal_end_page\": 4,\n",
    "                        \"type\": \"the second type of document detected\"\n",
    "                      }\n",
    "                    ]\n",
    "}\n",
    "```\"\"\"\n",
    "  },\n",
    "  \"extraction\": {\n",
    "    \"temperature\": \"0\",\n",
    "    \"model\": \"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"system_prompt\": \"You are a document assistant. Respond only with JSON. Never make up data, only provide data found in the document being provided.\\n\",\n",
    "    \"top_k\": \"200\",\n",
    "    \"task_prompt\": \"<background>\\nYou are an expert in business document analysis and information extraction. \\nYou can understand and extract key information from business documents classified as type \\n{DOCUMENT_CLASS}.\\n</background>\\n<document_ocr_data>\\n{DOCUMENT_TEXT}\\n</document_ocr_data>\\n<task>\\nYour task is to take the unstructured text provided and convert it into a well-organized table format using JSON. Identify the main entities, attributes, or categories mentioned in the attributes list below and use them as keys in the JSON object. \\nThen, extract the relevant information from the text and populate the corresponding values in the JSON object. \\nGuidelines:\\nEnsure that the data is accurately represented and properly formatted within the JSON structure\\nInclude double quotes around all keys and values\\nDo not make up data - only extract information explicitly found in the document\\nDo not use /n for new lines, use a space instead\\nIf a field is not found or if unsure, return null\\nAll dates should be in MM/DD/YYYY format\\nDo not perform calculations or summations unless totals are explicitly given\\nIf an alias is not found in the document, return null\\nHere are the attributes you should extract:\\n<attributes>\\n{ATTRIBUTE_NAMES_AND_DESCRIPTIONS}\\n</attributes>\\n</task>\\n\"\n",
    "  }\n",
    "}\n",
    "\n",
    "print(\"Test configuration created for IDP services with textbased holistic classification method enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Document with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.ocr.service:OCR Service initialized with features: ['LAYOUT']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created document with ID: doc-insurance-package\n",
      "Status: QUEUED\n",
      "\n",
      "Processing document with OCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 3\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 10\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 2\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 9\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 1\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 4\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 5\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 7\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 8\n",
      "INFO:idp_common.ocr.service:Successfully extracted markdown text for page 6\n",
      "INFO:idp_common.ocr.service:Sorting 10 pages by page number\n",
      "INFO:idp_common.ocr.service:OCR processing completed in 6.00 seconds\n",
      "INFO:idp_common.ocr.service:Processed 10 pages, with 0 errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR processing completed in 6.00 seconds\n",
      "Document status: OCR_COMPLETED\n",
      "Number of pages processed: 10\n",
      "\n",
      "Processed pages:\n",
      "Page 1:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/1/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/1/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/1/result.json\n",
      "Page 2:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/2/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/2/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/2/result.json\n",
      "Page 3:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/3/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/3/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/3/result.json\n",
      "Page 4:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/4/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/4/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/4/result.json\n",
      "Page 5:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/5/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/5/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/5/result.json\n",
      "Page 6:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/6/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/6/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/6/result.json\n",
      "Page 7:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/7/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/7/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/7/result.json\n",
      "Page 8:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/8/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/8/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/8/result.json\n",
      "Page 9:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/9/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/9/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/9/result.json\n",
      "Page 10:\n",
      "  Image URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/10/image.jpg\n",
      "  Raw Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/10/rawText.json\n",
      "  Parsed Text URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/pages/10/result.json\n",
      "\n",
      "Metering:\n",
      "{\"textract/analyze_document-Layout\": {\"pages\": 10}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new Document\n",
    "document = Document(\n",
    "    id=\"doc-insurance-package\",\n",
    "    input_bucket=input_bucket_name,\n",
    "    input_key=sample_file_key,\n",
    "    output_bucket=output_bucket_name,\n",
    "    status=Status.QUEUED\n",
    ")\n",
    "\n",
    "print(f\"Created document with ID: {document.id}\")\n",
    "print(f\"Status: {document.status.value}\")\n",
    "\n",
    "# Create OCR service with Textract\n",
    "# Valid features are 'LAYOUT', 'FORMS', 'SIGNATURES', 'TABLES' (uses analyze_document API)\n",
    "# or leave it empty (to use basic detect_document_text API)\n",
    "ocr_service = ocr.OcrService(\n",
    "    region=region,\n",
    "    enhanced_features=['LAYOUT']\n",
    ")\n",
    "\n",
    "# Process document with OCR\n",
    "print(\"\\nProcessing document with OCR...\")\n",
    "start_time = time.time()\n",
    "document = ocr_service.process_document(document)\n",
    "ocr_time = time.time() - start_time\n",
    "\n",
    "print(f\"OCR processing completed in {ocr_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of pages processed: {document.num_pages}\")\n",
    "\n",
    "# Show pages information\n",
    "print(\"\\nProcessed pages:\")\n",
    "for page_id, page in document.pages.items():\n",
    "    print(f\"Page {page_id}:\")\n",
    "    print(f\"  Image URI: {page.image_uri}\")\n",
    "    print(f\"  Raw Text URI: {page.raw_text_uri}\")\n",
    "    print(f\"  Parsed Text URI: {page.parsed_text_uri}\")\n",
    "print(\"\\nMetering:\")\n",
    "print(json.dumps(document.metering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classify the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.classification.service:Initialized classification service with Bedrock backend using model us.anthropic.claude-3-haiku-20240307-v1:0\n",
      "INFO:idp_common.classification.service:Using textbased holistic packet classification method\n",
      "INFO:idp_common.classification.service:Classifying document with 10 pages using holistic packet method\n",
      "INFO:idp_common.classification.service:Classifying document with 10 pages using holistic packet method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG classificationMethod: textbasedHolisticClassification\n",
      "\n",
      "Classifying document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.classification.service:Invoking Bedrock for holistic packet classification\n",
      "INFO:idp_common.classification.service:Time taken for holistic classification: 5.33 seconds\n",
      "WARNING:idp_common.classification.service:Unknown document type 'lab_report', using anyway\n",
      "INFO:idp_common.classification.service:Document classified with 8 sections using holistic method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification completed in 5.34 seconds\n",
      "Document status: CLASSIFIED\n",
      "\n",
      "Detected sections:\n",
      "Section 1: letter\n",
      "  Pages: ['1']\n",
      "Section 2: lab_report\n",
      "  Pages: ['2']\n",
      "Section 3: email\n",
      "  Pages: ['3']\n",
      "Section 4: invoice\n",
      "  Pages: ['4']\n",
      "Section 5: invoice\n",
      "  Pages: ['5']\n",
      "Section 6: scientific_publication\n",
      "  Pages: ['6']\n",
      "Section 7: questionnaire\n",
      "  Pages: ['7']\n",
      "Section 8: memo\n",
      "  Pages: ['8', '9', '10']\n",
      "\n",
      "Page-level classifications:\n",
      "Page 1: letter\n",
      "Page 10: memo\n",
      "Page 2: lab_report\n",
      "Page 3: email\n",
      "Page 4: invoice\n",
      "Page 5: invoice\n",
      "Page 6: scientific_publication\n",
      "Page 7: questionnaire\n",
      "Page 8: memo\n",
      "Page 9: memo\n"
     ]
    }
   ],
   "source": [
    "# Verify that Config specifies => \"classificationMethod\": \"textbasedHolisticClassification\"\n",
    "print(\"*****************************************************************\")\n",
    "print(f'CONFIG classificationMethod: {CONFIG[\"classification\"].get(\"classificationMethod\")}')\n",
    "print(\"*****************************************************************\")\n",
    "\n",
    "# Create classification service with Bedrock backend\n",
    "# The classification method is set in the config\n",
    "classification_service = classification.ClassificationService(\n",
    "    config=CONFIG, \n",
    "    backend=\"bedrock\" \n",
    ")\n",
    "\n",
    "# Classify the document\n",
    "print(\"\\nClassifying document...\")\n",
    "start_time = time.time()\n",
    "document = classification_service.classify_document(document)\n",
    "classification_time = time.time() - start_time\n",
    "print(f\"Classification completed in {classification_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "\n",
    "# Show classification results\n",
    "if document.sections:\n",
    "    print(\"\\nDetected sections:\")\n",
    "    for section in document.sections:\n",
    "        print(f\"Section {section.section_id}: {section.classification}\")\n",
    "        print(f\"  Pages: {section.page_ids}\")\n",
    "else:\n",
    "    print(\"\\nNo sections detected\")\n",
    "\n",
    "# Show page classification\n",
    "print(\"\\nPage-level classifications:\")\n",
    "for page_id, page in sorted(document.pages.items()):\n",
    "    print(f\"Page {page_id}: {page.classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Information from Document Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting information from document sections...\n",
      "\n",
      "Processing section 1 (class: letter)\n",
      "Extraction completed in 2.61 seconds\n",
      "Extraction result URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/1/result.json\n",
      "Processed 1 sections\n",
      "\n",
      "Processing section 2 (class: lab_report)\n",
      "Extraction completed in 6.03 seconds\n",
      "Extraction result URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/2/result.json\n",
      "Processed 2 sections\n",
      "\n",
      "Processing section 3 (class: email)\n",
      "Extraction completed in 2.45 seconds\n",
      "Extraction result URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/3/result.json\n",
      "Processed 3 sections\n",
      "\n",
      "Extraction for first 3 sections complete.\n",
      "\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# Create extraction service with Bedrock\n",
    "extraction_service = extraction.ExtractionService(config=CONFIG)\n",
    "\n",
    "print(\"\\nExtracting information from document sections...\")\n",
    "extracted_results = {}\n",
    "\n",
    "# Create individual document for each section\n",
    "n=0\n",
    "for section in document.sections:\n",
    "    print(f\"\\nProcessing section {section.section_id} (class: {section.classification})\")\n",
    "    \n",
    "    # Create a section-specific document\n",
    "    section_document = Document(\n",
    "        id=document.id,\n",
    "        input_bucket=document.input_bucket,\n",
    "        input_key=document.input_key,\n",
    "        output_bucket=document.output_bucket,\n",
    "        status=document.status,\n",
    "        sections=[section]\n",
    "    )\n",
    "    \n",
    "    # Add only pages needed for this section\n",
    "    needed_pages = {}\n",
    "    for page_id in section.page_ids:\n",
    "        if page_id in document.pages:\n",
    "            needed_pages[page_id] = document.pages[page_id]\n",
    "    section_document.pages = needed_pages\n",
    "    \n",
    "    # Process section\n",
    "    start_time = time.time()\n",
    "    section_document = extraction_service.process_document_section(\n",
    "        document=section_document,\n",
    "        section_id=section.section_id\n",
    "    )\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extraction completed in {extraction_time:.2f} seconds\")\n",
    "    \n",
    "    # Get the updated section\n",
    "    updated_section = section_document.sections[0]\n",
    "    print(f\"Extraction result URI: {updated_section.extraction_result_uri}\")\n",
    "    \n",
    "    # Store results for later use\n",
    "    extracted_results[section.section_id] = {\n",
    "        \"section\": updated_section,\n",
    "        \"result_uri\": updated_section.extraction_result_uri\n",
    "    }\n",
    "    n += 1\n",
    "    print(f\"Processed {n} sections\")\n",
    "    if n >= 3:  # Limit to first 3 sections to save time/costs\n",
    "        print(\"\\nExtraction for first 3 sections complete.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nExtraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extraction results from S3...\n",
      "\n",
      "Results for section 1 (class: letter)\n",
      "S3 URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/1/result.json\n",
      "Extracted attributes:\n",
      "  sender_name: Will E. Clark\n",
      "  sender_address: 206 Maple Street P.O. Box 1056 Murray Kentucky 42071-1056\n",
      "\n",
      "Results for section 2 (class: lab_report)\n",
      "S3 URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/2/result.json\n",
      "Extracted attributes:\n",
      "  DATE: 2/28/93\n",
      "  TECHNICIAN: CC\n",
      "  SHIFT: A\n",
      "  LINE: 2\n",
      "  AREA: 52\n",
      "  PRODUCT UNIT CODE: 0728\n",
      "  SAMPLE ID: stuff box 2\n",
      "  REASON FOR REQUEST: test\n",
      "  REQUESTED DELIVERY TIME: None\n",
      "  TIME SAMPLE RECEIVED: None\n",
      "  TIME ANALYSIS COMPLETED: None\n",
      "  DATA COMMUNICATED TO: Gone\n",
      "  DATA COMMUNICATED AT: 11:05\n",
      "  DRYING TIME: {'IN': None, 'OUT': None}\n",
      "  SAMPLE & CONTAINER WEIGHT IN GRAMS: 1159.3\n",
      "  CONTAINER WEIGHT IN GRAMS: 6\n",
      "  SAMPLE WEIGHT IN GRAMS: 2\n",
      "  DILUTION FACTOR: 6955.8\n",
      "  DILUTED SAMPLE WEIGHT IN GRAMS: None\n",
      "  FILTER PAPER WEIGHT IN GRAMS: 2645\n",
      "  SAMPLE & FILTER PAPER WEIGHT IN GRAMS WET: 2853\n",
      "  SAMPLE & FILTER PAPER WEIGHT IN GRAMS DRY: 2647\n",
      "  CONSISTENCY AVERAGE: 3.68\n",
      "  CONSISTENCY RANGE: 15\n",
      "  METER READING: None\n",
      "  COMMENTS: None\n",
      "\n",
      "Results for section 3 (class: email)\n",
      "S3 URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/3/result.json\n",
      "Extracted attributes:\n",
      "  from_address: Kelahan, Ben\n",
      "  to_address: TI New York: 'TI Minnesota Co: Ashley Bratich (MSMAIL)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading extraction results from S3...\\n\")\n",
    "\n",
    "for section_id, data in extracted_results.items():\n",
    "    # Load the extraction results from S3\n",
    "    uri = data['result_uri']\n",
    "    try:\n",
    "        result_data = load_json_from_s3(uri)\n",
    "        \n",
    "        # Extract the inference results\n",
    "        if \"inference_result\" in result_data:\n",
    "            extraction_results = result_data[\"inference_result\"]\n",
    "        else:\n",
    "            extraction_results = result_data\n",
    "            \n",
    "        # Print out section and extraction results\n",
    "        print(f\"Results for section {section_id} (class: {data['section'].classification})\")\n",
    "        print(f\"S3 URI: {uri}\")\n",
    "        print(\"Extracted attributes:\")\n",
    "        for key, value in extraction_results.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results from {uri}: {str(e)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Document Status Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Document State:\n",
      "Document ID: doc-insurance-package\n",
      "Status: PROCESSED\n",
      "Number of pages: 10\n",
      "Number of sections: 9\n",
      "\n",
      "Section summary:\n",
      "  Section 1: letter\n",
      "    Pages: ['1']\n",
      "    Extraction result URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/1/result.json\n",
      "  Section 2: lab_report\n",
      "    Pages: ['2']\n",
      "    Extraction result URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/2/result.json\n",
      "  Section 3: email\n",
      "    Pages: ['3']\n",
      "    Extraction result URI: s3://idp-holistic-output-912625584728-us-west-2/sample-2025-04-14_15-27-10.pdf/sections/3/result.json\n",
      "  Section 4: memo\n",
      "    Pages: ['4']\n",
      "    Extraction result URI: None\n",
      "  Section 5: invoice\n",
      "    Pages: ['5']\n",
      "    Extraction result URI: None\n",
      "  Section 6: scientific_publication\n",
      "    Pages: ['6']\n",
      "    Extraction result URI: None\n",
      "  Section 7: news_release\n",
      "    Pages: ['7', '8']\n",
      "    Extraction result URI: None\n",
      "  Section 8: questionnaire\n",
      "    Pages: ['9']\n",
      "    Extraction result URI: None\n",
      "  Section 9: biographical_sketch\n",
      "    Pages: ['10']\n",
      "    Extraction result URI: None\n",
      "\n",
      "Document can be serialized to JSON:\n",
      "{\n",
      "  \"id\": \"doc-insurance-package\",\n",
      "  \"input_bucket\": \"idp-holistic-input-912625584728-us-west-2\",\n",
      "  \"input_key\": \"sample-2025-04-14_15-27-10.pdf\",\n",
      "  \"output_bucket\": \"idp-holistic-output-912625584728-us-west-2\",\n",
      "  \"status\": \"PROCESSED\",\n",
      "  \"queued_time\": null,\n",
      "  \"start_time\": null,\n",
      "  \"completion_time\": null,\n",
      "  \"workflow_execution_arn\": null,\n",
      "  \"num_pages\": 10,\n",
      "  \"evaluation_report_uri\": null,\n",
      "  \"errors\": [],\n",
      "  \"metering\": {\n",
      "    \"textract/analyze_document-Layout\": {\n",
      "      \"pages\": 10\n",
      "    },\n",
      "    \"b...\n",
      "(truncated for display)\n"
     ]
    }
   ],
   "source": [
    "# Update document status to PROCESSED\n",
    "document.status = Status.PROCESSED\n",
    "\n",
    "# Update document sections with extraction results\n",
    "for section_id, data in extracted_results.items():\n",
    "    # Find section in document\n",
    "    for i, section in enumerate(document.sections):\n",
    "        if section.section_id == section_id:\n",
    "            document.sections[i] = data['section']\n",
    "\n",
    "# Display final document state\n",
    "print(\"Final Document State:\")\n",
    "print(f\"Document ID: {document.id}\")\n",
    "print(f\"Status: {document.status.value}\")\n",
    "print(f\"Number of pages: {document.num_pages}\")\n",
    "print(f\"Number of sections: {len(document.sections)}\")\n",
    "\n",
    "print(\"\\nSection summary:\")\n",
    "for section in document.sections:\n",
    "    print(f\"  Section {section.section_id}: {section.classification}\")\n",
    "    print(f\"    Pages: {section.page_ids}\")\n",
    "    print(f\"    Extraction result URI: {section.extraction_result_uri}\")\n",
    "\n",
    "# Demonstrate that a document can be serialized to JSON\n",
    "print(\"\\nDocument can be serialized to JSON:\")\n",
    "document_dict = document.to_dict()\n",
    "document_json = json.dumps(document_dict, indent=2)[:500]  # Truncate for display\n",
    "print(f\"{document_json}...\")\n",
    "print(\"(truncated for display)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete objects in a bucket\n",
    "def delete_bucket_objects(bucket_name):\n",
    "    try:\n",
    "        # List all objects in the bucket\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            delete_keys = {'Objects': [{'Key': obj['Key']} for obj in response['Contents']]}\n",
    "            s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\n",
    "            print(f\"Deleted all objects in bucket {bucket_name}\")\n",
    "        else:\n",
    "            print(f\"Bucket {bucket_name} is already empty\")\n",
    "            \n",
    "        # Delete bucket\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"Deleted bucket {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up bucket {bucket_name}: {str(e)}\")\n",
    "\n",
    "# Uncomment the following lines to delete the buckets\n",
    "# print(\"Cleaning up resources...\")\n",
    "# delete_bucket_objects(input_bucket_name)\n",
    "# delete_bucket_objects(output_bucket_name)\n",
    "# print(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to use the holistic packet classification capability in the IDP Common Package, which offers several advantages over page-by-page classification:\n",
    "\n",
    "### Benefits of Holistic Packet Classification:\n",
    "\n",
    "1. **Contextual Understanding**: Analyzes the entire document as a whole, rather than page-by-page, providing better context for classification decisions\n",
    "2. **Document Boundary Detection**: Precisely detects where one document ends and another begins within a multi-document packet\n",
    "3. **Improved Accuracy**: More accurately classifies pages that might be ambiguous when viewed individually\n",
    "4. **Type Consistency**: Maintains document type consistency for multi-page documents\n",
    "5. **Flexible Configuration**: Supports customizable prompts and document type definitions\n",
    "\n",
    "Compared to page-by-page classification (multimodalPageLevelClassification), the textbased holistic classification (textbasedHolisticClassification) approach gives more accurate results for complex document packets, especially when individual pages might not provide enough context for accurate classification on their own.\n",
    "\n",
    "The implementation leverages the existing Document model structure, where:\n",
    "- Document = DocumentPacket\n",
    "- Section = DocumentSegment\n",
    "\n",
    "This approach ensures consistency with the rest of the IDP Common Package while adding the powerful capability of holistic document packet classification. The classification method can be configured through the \"classificationMethod\" parameter in the configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
